# transcriber.py
import os
import google.generativeai as genai
from faster_whisper import WhisperModel
from pathlib import Path

# å¯é€‰æ¨¡å‹ï¼ˆä»å°åˆ°å¤§ï¼‰: tiny (~75MB), base (~150MB), small (~500MB), medium (~1.5GB), large-v3 (~3GB)
# æ¨¡å‹è¶Šå¤§ï¼Œè½¬å½•è¶Šå‡†ç¡®ï¼Œä½†ä¸‹è½½å’Œè¿è¡Œæ—¶é—´è¶Šé•¿
MODEL_SIZE = "base"

# Gemini API Key
GEMINI_API_KEY = "AIzaSyCnZ8s1hnroyOEsQ8oUjb7sOt0OmCoPOlU"


# æ¯æ®µå‘ç»™ Gemini çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆçº¦ 8000 å­—ï¼Œç•™è¶³ä½™é‡é¿å…è¶…æ—¶ï¼‰
CHUNK_SIZE = 8000

PROMPT_TEMPLATE = (
    "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬ç¼–è¾‘åŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ä¸€æ®µè¯­éŸ³è½¬å½•çš„åŸå§‹æ–‡æœ¬ï¼ˆæ²¡æœ‰åˆ†æ®µï¼‰ã€‚"
    "è¯·ä½ å…ˆåˆ¤æ–­è¿™æ®µå†…å®¹æ˜¯ã€Œå¤šäººå¯¹è¯ã€è¿˜æ˜¯ã€Œå•äººç‹¬ç™½ã€ï¼Œç„¶åæŒ‰å¯¹åº”æ ¼å¼æ•´ç†ä¸º Markdownã€‚\n\n"
    "**å¦‚æœæ˜¯å¤šäººå¯¹è¯ï¼ˆè®¿è°ˆã€æ’­å®¢ã€èŠå¤©ç­‰ï¼‰ï¼š**\n"
    "1. è¯†åˆ«ä¸åŒçš„è¯´è¯äººï¼Œç”¨ **è¯´è¯äººAï¼š**ã€**è¯´è¯äººBï¼š** ç­‰æ ‡è®°ï¼ˆå¦‚æœèƒ½ä»å†…å®¹æ¨æ–­å‡ºåå­—åˆ™ç”¨åå­—ï¼‰\n"
    "2. æ¯æ¬¡è¯´è¯äººåˆ‡æ¢æ—¶æ¢è¡Œï¼Œå¦‚å®è®°å½•æ¯ä¸ªäººè¯´çš„è¯\n"
    "3. ä¸éœ€è¦æ·»åŠ å°æ ‡é¢˜ï¼Œä¸éœ€è¦åˆå¹¶æ®µè½ï¼Œå¿ å®è¿˜åŸå¯¹è¯è¿‡ç¨‹\n\n"
    "**å¦‚æœæ˜¯å•äººç‹¬ç™½ï¼ˆæ¼”è®²ã€vlogã€è®²è§£ç­‰ï¼‰ï¼š**\n"
    "1. æŒ‰ç…§è¯é¢˜å’Œé€»è¾‘è½¬æŠ˜æ¥åˆ†æ®µï¼Œä¸è¦æœºæ¢°åœ°æŒ‰å›ºå®šå­—æ•°åˆ†\n"
    "2. ç”¨ ## ä¸ºæ¯ä¸ªä¸»è¦è¯é¢˜æ®µè½æ·»åŠ å°æ ‡é¢˜ï¼ˆå°æ ‡é¢˜ç”±ä½ æ ¹æ®å†…å®¹æ€»ç»“ï¼‰\n"
    "3. æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œéš”å¼€\n\n"
    "**é€šç”¨è¦æ±‚ï¼š**\n"
    "1. å°†æ‰€æœ‰ç¹ä½“ä¸­æ–‡è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡\n"
    "2. é™¤äº†ç¹ç®€è½¬æ¢å¤–ï¼Œä¸è¦ä¿®æ”¹ã€åˆ é™¤æˆ–æ·»åŠ ä»»ä½•åŸæ–‡å†…å®¹\n"
    "3. ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ï¼Œä¸è¦åŠ é¢å¤–è¯´æ˜\n\n"
)


def _split_text(text: str, chunk_size: int = CHUNK_SIZE) -> list[str]:
    """å°†é•¿æ–‡æœ¬æŒ‰å¥å·/é—®å·/æ„Ÿå¹å·ç­‰æ–­å¥ç‚¹åˆ‡åˆ†ä¸ºå¤šæ®µï¼Œæ¯æ®µä¸è¶…è¿‡ chunk_size å­—ç¬¦"""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        # å¦‚æœå‰©ä½™æ–‡æœ¬ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥æ”¶å…¥
        if start + chunk_size >= len(text):
            chunks.append(text[start:])
            break

        # åœ¨ chunk_size èŒƒå›´å†…ä»åå¾€å‰æ‰¾æ–­å¥ç‚¹
        end = start + chunk_size
        split_pos = -1
        for sep in ["ã€‚", "ï¼Ÿ", "ï¼", ".", "?", "!", "ï¼›", "\n"]:
            pos = text.rfind(sep, start, end)
            if pos > split_pos:
                split_pos = pos

        if split_pos > start:
            # åŒ…å«æ–­å¥ç¬¦å·æœ¬èº«
            chunks.append(text[start:split_pos + 1])
            start = split_pos + 1
        else:
            # æ‰¾ä¸åˆ°æ–­å¥ç‚¹ï¼Œç¡¬åˆ‡
            chunks.append(text[start:end])
            start = end

    return chunks


def _call_gemini(api_key: str, text: str, part_info: str = "") -> str:
    """è°ƒç”¨ Gemini API æ ¼å¼åŒ–ä¸€æ®µæ–‡æœ¬ï¼Œå¸¦é‡è¯•"""
    import time

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash")

    prompt = PROMPT_TEMPLATE + f"åŸå§‹æ–‡æœ¬ï¼š\n{text}"

    max_retries = 3
    for attempt in range(1, max_retries + 1):
        try:
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(temperature=0),
                request_options={"timeout": 600},
            )
            return response.text.strip()
        except Exception as e:
            if attempt < max_retries:
                wait = attempt * 10
                print(f"   â³ {part_info}ç¬¬ {attempt} æ¬¡è¯·æ±‚å¤±è´¥ï¼ˆ{e}ï¼‰ï¼Œ{wait}s åé‡è¯•...")
                time.sleep(wait)
            else:
                raise


def _format_with_llm(raw_text: str) -> str:
    """ä½¿ç”¨ Gemini æŒ‰é€»è¾‘å†…å®¹å¯¹æ–‡æœ¬è¿›è¡Œåˆ†æ®µï¼Œè¶…é•¿æ–‡æœ¬è‡ªåŠ¨åˆ†æ®µå¤„ç†"""
    api_key = os.environ.get("GEMINI_API_KEY", GEMINI_API_KEY)

    chunks = _split_text(raw_text)

    if len(chunks) == 1:
        print("âœ¨ æ­£åœ¨ç”¨ Gemini AI æŒ‰é€»è¾‘å†…å®¹åˆ†æ®µ...")
        return _call_gemini(api_key, chunks[0])
    else:
        print(f"âœ¨ æ–‡æœ¬è¾ƒé•¿ï¼ˆ{len(raw_text)} å­—ï¼‰ï¼Œåˆ† {len(chunks)} æ®µå‘é€ç»™ Gemini AI...")
        results = []
        for i, chunk in enumerate(chunks, 1):
            print(f"   ğŸ“ å¤„ç†ç¬¬ {i}/{len(chunks)} æ®µï¼ˆ{len(chunk)} å­—ï¼‰...")
            result = _call_gemini(api_key, chunk, part_info=f"ç¬¬{i}æ®µ ")
            results.append(result)
        return "\n\n".join(results)


def transcribe(audio_path: Path, output_dir: str = "output"):
    print(f"ğŸ“¦ åŠ è½½ Whisper æ¨¡å‹ ({MODEL_SIZE})ï¼Œé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    model = WhisperModel(
        MODEL_SIZE,
        device="cpu",
        compute_type="int8"
    )
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹è½¬å½•...")

    segments, info = model.transcribe(str(audio_path), language="zh")

    # ä½¿ç”¨ä¸éŸ³é¢‘ç›¸åŒçš„æ–‡ä»¶åï¼ˆå»æ‰ .mp3 åç¼€ï¼ŒåŠ  .mdï¼‰
    transcript_path = Path(output_dir) / f"{audio_path.stem}.md"

    # å…ˆæ‹¼æ¥ä¸ºå®Œæ•´æ–‡æœ¬
    raw_parts = []
    for segment in segments:
        text = segment.text.strip()
        if text:
            print(f"  [{segment.start:.1f}s - {segment.end:.1f}s] {text}")
            raw_parts.append(text)

    raw_text = "".join(raw_parts)
    print(f"\nğŸ“ è½¬å½•å®Œæˆï¼Œå…± {len(raw_text)} å­—")

    # ç”¨ Gemini æŒ‰é€»è¾‘å†…å®¹åˆ†æ®µå¹¶ç”Ÿæˆ Markdown
    formatted_text = _format_with_llm(raw_text)

    # ç”¨æ–‡ä»¶åï¼ˆå³è§†é¢‘æ ‡é¢˜ï¼‰ä½œä¸ºä¸€çº§æ ‡é¢˜
    title = audio_path.stem.replace("_", " ")

    with open(transcript_path, "w", encoding="utf-8") as f:
        f.write(f"# {title}\n\n{formatted_text}\n")

    return transcript_path
