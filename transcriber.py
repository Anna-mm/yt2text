# transcriber.py
import os
import time
import concurrent.futures
from openai import OpenAI
from faster_whisper import WhisperModel
from pathlib import Path

# å¯é€‰æ¨¡å‹ï¼ˆä»å°åˆ°å¤§ï¼‰: tiny (~75MB), base (~150MB), small (~500MB), medium (~1.5GB), large-v3 (~3GB)
# æ¨¡å‹è¶Šå¤§ï¼Œè½¬å½•è¶Šå‡†ç¡®ï¼Œä½†ä¸‹è½½å’Œè¿è¡Œæ—¶é—´è¶Šé•¿
MODEL_SIZE = "base"

# é€šä¹‰åƒé—® API Keyï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼Œæ–°ç”¨æˆ·æ¯ä¸ªæ¨¡å‹é€ 100 ä¸‡ tokensï¼Œæœ‰æ•ˆæœŸ 90 å¤©ï¼‰
# è·å–åœ°å€ï¼šhttps://bailian.console.aliyun.com/#/api-key
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY", "sk-a6291c230f014c7491b3a27a0f347b7f")
DASHSCOPE_MODEL = "qwen-turbo"
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

# æ¯æ®µå‘ç»™ LLM çš„æœ€å¤§å­—ç¬¦æ•°
CHUNK_SIZE = 8000

PROMPT_TEMPLATE = (
    "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬ç¼–è¾‘åŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ä¸€æ®µè¯­éŸ³è½¬å½•çš„åŸå§‹æ–‡æœ¬ï¼ˆæ²¡æœ‰åˆ†æ®µï¼‰ã€‚"
    "è¯·ä½ å…ˆåˆ¤æ–­è¿™æ®µå†…å®¹æ˜¯ã€Œå¤šäººå¯¹è¯ã€è¿˜æ˜¯ã€Œå•äººç‹¬ç™½ã€ï¼Œç„¶åæŒ‰å¯¹åº”æ ¼å¼æ•´ç†ä¸º Markdownã€‚\n\n"
    "**å¦‚æœæ˜¯å¤šäººå¯¹è¯ï¼ˆè®¿è°ˆã€æ’­å®¢ã€èŠå¤©ç­‰ï¼‰ï¼š**\n"
    "1. è¯†åˆ«ä¸åŒçš„è¯´è¯äººï¼Œç”¨ **è¯´è¯äººAï¼š**ã€**è¯´è¯äººBï¼š** ç­‰æ ‡è®°ï¼ˆå¦‚æœèƒ½ä»å†…å®¹æ¨æ–­å‡ºåå­—åˆ™ç”¨åå­—ï¼‰\n"
    "2. æ¯æ¬¡è¯´è¯äººåˆ‡æ¢æ—¶æ¢è¡Œï¼Œå¦‚å®è®°å½•æ¯ä¸ªäººè¯´çš„è¯\n"
    "3. ä¸éœ€è¦æ·»åŠ å°æ ‡é¢˜ï¼Œä¸éœ€è¦åˆå¹¶æ®µè½ï¼Œå¿ å®è¿˜åŸå¯¹è¯è¿‡ç¨‹\n\n"
    "**å¦‚æœæ˜¯å•äººç‹¬ç™½ï¼ˆæ¼”è®²ã€vlogã€è®²è§£ç­‰ï¼‰ï¼š**\n"
    "1. æŒ‰ç…§è¯é¢˜å’Œé€»è¾‘è½¬æŠ˜æ¥åˆ†æ®µï¼Œä¸è¦æœºæ¢°åœ°æŒ‰å›ºå®šå­—æ•°åˆ†\n"
    "2. ç”¨ ## ä¸ºæ¯ä¸ªä¸»è¦è¯é¢˜æ®µè½æ·»åŠ å°æ ‡é¢˜ï¼ˆå°æ ‡é¢˜ç”±ä½ æ ¹æ®å†…å®¹æ€»ç»“ï¼‰\n"
    "3. æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œéš”å¼€\n\n"
    "**é€šç”¨è¦æ±‚ï¼š**\n"
    "1. å°†æ‰€æœ‰ç¹ä½“ä¸­æ–‡è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡\n"
    "2. é™¤äº†ç¹ç®€è½¬æ¢å¤–ï¼Œä¸è¦ä¿®æ”¹ã€åˆ é™¤æˆ–æ·»åŠ ä»»ä½•åŸæ–‡å†…å®¹\n"
    "3. ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ï¼Œä¸è¦åŠ é¢å¤–è¯´æ˜\n\n"
)

# Whisper æ¨¡å‹å•ä¾‹ç¼“å­˜
_whisper_model = None

def _get_whisper_model():
    """åŠ è½½å¹¶ç¼“å­˜ Whisper æ¨¡å‹ï¼Œåªåœ¨é¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½"""
    global _whisper_model
    if _whisper_model is None:
        print(f"ğŸ“¦ é¦–æ¬¡åŠ è½½ Whisper æ¨¡å‹ ({MODEL_SIZE})ï¼Œè¯·ç¨å€™...")
        _whisper_model = WhisperModel(MODEL_SIZE, device="cpu", compute_type="int8")
        print("âœ… Whisper æ¨¡å‹åŠ è½½å®Œæˆï¼ˆå·²ç¼“å­˜ï¼‰")
    return _whisper_model


SEGMENT_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬ç¼–è¾‘åŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ä¸€æ®µè¯­éŸ³è½¬å½•çš„åŸå§‹æ–‡æœ¬ç‰‡æ®µï¼ˆç”±è¯­éŸ³è¯†åˆ«é€å¥ç”Ÿæˆï¼Œå¥å­é›¶æ•£ï¼‰ã€‚\n"
    "è¯·å°†è¿™äº›é›¶æ•£çš„çŸ­å¥æ•´ç†æˆè¿è´¯ã€æµç•…çš„æ®µè½ã€‚å…·ä½“è¦æ±‚ï¼š\n"
    "1. å°†ç¹ä½“ä¸­æ–‡è½¬ä¸ºç®€ä½“ä¸­æ–‡\n"
    "2. æŠŠé›¶æ•£çš„çŸ­å¥åˆå¹¶æˆå®Œæ•´çš„ã€è¿è´¯çš„é•¿æ®µè½ï¼Œæ·»åŠ é€‚å½“çš„æ ‡ç‚¹ç¬¦å·\n"
    "3. æŒ‰è¯­ä¹‰å’Œè¯é¢˜ç»„ç»‡æ®µè½ï¼Œæ®µè½ä¹‹é—´ç”¨ç©ºè¡Œéš”å¼€\n"
    "4. ä¸è¦æ·»åŠ å°æ ‡é¢˜ï¼ˆæ ‡é¢˜ç”±å¤–éƒ¨ç»Ÿä¸€å¤„ç†ï¼‰\n"
    "5. ä¸è¦åˆ é™¤æˆ–æ·»åŠ åŸæ–‡çš„å®é™…å†…å®¹ï¼Œåªåšæ ¼å¼æ•´ç†\n"
    "6. ç›´æ¥è¾“å‡ºæ–‡æœ¬ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ï¼Œä¸è¦åŠ é¢å¤–è¯´æ˜\n\n"
)

# AI è‡ªåŠ¨ç”Ÿæˆç»“æ„æ ‡é¢˜çš„ promptï¼ˆç”¨äºæ²¡æœ‰ YouTube ç« èŠ‚çš„è§†é¢‘ï¼‰
STRUCTURE_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬ç»“æ„åŒ–åŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªè§†é¢‘è½¬å½•æ–‡æœ¬çš„å„æ®µè½æ‘˜è¦ï¼ˆæ¯æ®µåªæ˜¾ç¤ºå‰150å­—ï¼‰ã€‚\n"
    "è¯·åˆ†æè¿™äº›æ®µè½çš„å†…å®¹ï¼Œå°†å®ƒä»¬åˆ’åˆ†ä¸º 3-8 ä¸ªä¸»é¢˜æ¿å—ï¼Œä¸ºæ¯ä¸ªæ¿å—èµ·ä¸€ä¸ªç®€æ´çš„æ ‡é¢˜ã€‚\n\n"
    "è¾“å‡ºæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼š\n"
    "æ®µè½ç¼–å·:æ ‡é¢˜\n\n"
    "ä¾‹å¦‚ï¼š\n"
    "1:ä¸ªäººç»å†ä¸èƒŒæ™¯\n"
    "5:å›¢é˜Ÿç®¡ç†å¿ƒå¾—\n"
    "12:æ€»ç»“ä¸å±•æœ›\n\n"
    "è¦æ±‚ï¼š\n"
    "1. æ®µè½ç¼–å·è¡¨ç¤ºè¯¥ä¸»é¢˜ä»å“ªä¸ªæ®µè½å¼€å§‹ï¼ˆç¬¬ä¸€ä¸ªæ¿å—ä¸€èˆ¬ä»æ®µè½1å¼€å§‹ï¼‰\n"
    "2. æ ‡é¢˜è¦ç®€çŸ­ã€æ¦‚æ‹¬ï¼ˆ5-15ä¸ªå­—ï¼‰ï¼Œåƒ YouTube è§†é¢‘çš„ç« èŠ‚æ ‡é¢˜ä¸€æ ·\n"
    "3. åªè¾“å‡ºä¸Šè¿°æ ¼å¼ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–å†…å®¹\n"
)


def _split_text(text: str, chunk_size: int = CHUNK_SIZE) -> list[str]:
    """å°†é•¿æ–‡æœ¬æŒ‰å¥å·/é—®å·/æ„Ÿå¹å·ç­‰æ–­å¥ç‚¹åˆ‡åˆ†ä¸ºå¤šæ®µï¼Œæ¯æ®µä¸è¶…è¿‡ chunk_size å­—ç¬¦"""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        # å¦‚æœå‰©ä½™æ–‡æœ¬ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥æ”¶å…¥
        if start + chunk_size >= len(text):
            chunks.append(text[start:])
            break

        # åœ¨ chunk_size èŒƒå›´å†…ä»åå¾€å‰æ‰¾æ–­å¥ç‚¹
        end = start + chunk_size
        split_pos = -1
        for sep in ["ã€‚", "ï¼Ÿ", "ï¼", ".", "?", "!", "ï¼›", "\n"]:
            pos = text.rfind(sep, start, end)
            if pos > split_pos:
                split_pos = pos

        if split_pos > start:
            # åŒ…å«æ–­å¥ç¬¦å·æœ¬èº«
            chunks.append(text[start:split_pos + 1])
            start = split_pos + 1
        else:
            # æ‰¾ä¸åˆ°æ–­å¥ç‚¹ï¼Œç¡¬åˆ‡
            chunks.append(text[start:end])
            start = end

    return chunks


def _get_llm_client() -> OpenAI:
    """åˆ›å»ºé€šä¹‰åƒé—® API å®¢æˆ·ç«¯ï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ DashScopeï¼‰"""
    api_key = os.environ.get("DASHSCOPE_API_KEY", DASHSCOPE_API_KEY)
    if not api_key:
        raise RuntimeError(
            "æœªè®¾ç½®é€šä¹‰åƒé—® API Keyã€‚è¯·å‰å¾€ https://bailian.console.aliyun.com/#/api-key è·å–ï¼Œ"
            "ç„¶åè®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æˆ–åœ¨ transcriber.py ä¸­å¡«å†™ã€‚"
        )
    return OpenAI(api_key=api_key, base_url=DASHSCOPE_BASE_URL)


def _call_llm(client: OpenAI, text: str, part_info: str = "", prompt_template: str = None) -> str:
    """è°ƒç”¨é€šä¹‰åƒé—® API æ ¼å¼åŒ–ä¸€æ®µæ–‡æœ¬ï¼Œå¸¦é‡è¯•"""
    system_prompt = (prompt_template or PROMPT_TEMPLATE).rstrip()
    user_content = f"åŸå§‹æ–‡æœ¬ï¼š\n{text}"

    max_retries = 5
    for attempt in range(1, max_retries + 1):
        try:
            response = client.chat.completions.create(
                model=DASHSCOPE_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content},
                ],
                temperature=0,
                timeout=90,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            if attempt < max_retries:
                # ç½‘ç»œé”™è¯¯ç”¨æ›´é•¿ç­‰å¾…ï¼Œå…¶ä»–é”™è¯¯çŸ­ç­‰å¾…
                is_network = "connect" in str(e).lower() or "timeout" in str(e).lower()
                wait = attempt * 10 if is_network else attempt * 5
                print(f"   â³ {part_info}ç¬¬ {attempt} æ¬¡è¯·æ±‚å¤±è´¥ï¼ˆ{e}ï¼‰ï¼Œ{wait}s åé‡è¯•...")
                time.sleep(wait)
            else:
                print(f"   âš ï¸ {part_info}è¯·æ±‚å¤±è´¥: {e}")
                raise


def _generate_section_headers(client: OpenAI, paragraphs: list[dict]) -> dict[int, str]:
    """ä¸ºæ²¡æœ‰ YouTube ç« èŠ‚çš„è§†é¢‘ï¼Œç”¨ AI åˆ†ææ®µè½æ‘˜è¦ç”Ÿæˆç»“æ„åŒ–æ ‡é¢˜

    è¿”å›: {æ®µè½ç´¢å¼•: "æ ‡é¢˜", ...}ï¼ˆç´¢å¼•ä» 0 å¼€å§‹ï¼‰
    """
    if len(paragraphs) < 3:
        return {}

    # æ„å»ºæ®µè½æ‘˜è¦ï¼šå–æ¯æ®µå‰ 150 å­—
    summaries = []
    for i, p in enumerate(paragraphs):
        text = (p["formatted"] or p["raw"]).strip()
        preview = text[:150].replace("\n", " ").strip()
        summaries.append(f"ã€æ®µè½{i+1}ã€‘{preview}")

    outline = "\n".join(summaries)

    result = _call_llm(client, outline, part_info="ç»“æ„åŒ– ", prompt_template=STRUCTURE_PROMPT)

    # è§£æ AI è¿”å›çš„ "æ®µè½ç¼–å·:æ ‡é¢˜" æ ¼å¼
    headers = {}
    for line in result.strip().split("\n"):
        line = line.strip()
        if not line or ":" not in line and "ï¼š" not in line:
            continue
        # æ”¯æŒåŠè§’å’Œå…¨è§’å†’å·
        sep = "ï¼š" if "ï¼š" in line else ":"
        parts = line.split(sep, 1)
        try:
            num_str = parts[0].replace("æ®µè½", "").strip()
            idx = int(num_str) - 1  # è½¬ä¸º 0-indexed
            title = parts[1].strip()
            if 0 <= idx < len(paragraphs) and title:
                headers[idx] = title
        except (ValueError, IndexError):
            continue

    return headers


def _format_with_llm(raw_text: str) -> str:
    """ä½¿ç”¨é€šä¹‰åƒé—®æŒ‰é€»è¾‘å†…å®¹å¯¹æ–‡æœ¬è¿›è¡Œåˆ†æ®µï¼Œè¶…é•¿æ–‡æœ¬è‡ªåŠ¨åˆ†æ®µå¤„ç†"""
    client = _get_llm_client()

    chunks = _split_text(raw_text)

    if len(chunks) == 1:
        print("âœ¨ æ­£åœ¨ç”¨é€šä¹‰åƒé—® AI æŒ‰é€»è¾‘å†…å®¹åˆ†æ®µ...")
        return _call_llm(client, chunks[0])
    else:
        print(f"âœ¨ æ–‡æœ¬è¾ƒé•¿ï¼ˆ{len(raw_text)} å­—ï¼‰ï¼Œåˆ† {len(chunks)} æ®µå‘é€ç»™é€šä¹‰åƒé—® AI...")
        results = []
        for i, chunk in enumerate(chunks, 1):
            print(f"   ğŸ“ å¤„ç†ç¬¬ {i}/{len(chunks)} æ®µï¼ˆ{len(chunk)} å­—ï¼‰...")
            result = _call_llm(client, chunk, part_info=f"ç¬¬{i}æ®µ ")
            results.append(result)
        return "\n\n".join(results)


def transcribe_audio(audio_path: Path, on_progress=None) -> str:
    """ç¬¬ä¸€é˜¶æ®µï¼šç”¨ Whisper å°†éŸ³é¢‘è½¬ä¸ºåŸå§‹æ–‡æœ¬ï¼Œæ”¯æŒé€æ®µå›è°ƒ"""
    model = _get_whisper_model()
    print("âœ… æ¨¡å‹å°±ç»ªï¼Œå¼€å§‹è½¬å½•...")

    segments, info = model.transcribe(str(audio_path), language="zh")

    # è¯­éŸ³ä¸­è¶…è¿‡ GAP_THRESHOLD ç§’çš„åœé¡¿ä¼šè‡ªåŠ¨åˆ†æ®µï¼ˆæ’å…¥ç©ºè¡Œï¼‰
    GAP_THRESHOLD = 1.0
    raw_parts = []
    prev_end = 0.0
    for segment in segments:
        text = segment.text.strip()
        if text:
            gap = segment.start - prev_end
            if raw_parts and gap >= GAP_THRESHOLD:
                raw_parts.append("\n\n")
                print(f"  --- åœé¡¿ {gap:.1f}sï¼Œåˆ†æ®µ ---")
            print(f"  [{segment.start:.1f}s - {segment.end:.1f}s] {text}")
            raw_parts.append(text)
            prev_end = segment.end
            if on_progress:
                on_progress("".join(raw_parts))

    raw_text = "".join(raw_parts)
    print(f"\nğŸ“ è½¬å½•å®Œæˆï¼Œå…± {len(raw_text)} å­—")
    return raw_text


def format_transcript(raw_text: str, audio_path: Path, output_dir: str = "output") -> Path:
    """ç¬¬äºŒé˜¶æ®µï¼šç”¨é€šä¹‰åƒé—® AI å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œé€»è¾‘åˆ†æ®µå¹¶ç”Ÿæˆ Markdown"""
    transcript_path = Path(output_dir) / f"{audio_path.stem}.md"

    formatted_text = _format_with_llm(raw_text)

    title = audio_path.stem.replace("_", " ")

    with open(transcript_path, "w", encoding="utf-8") as f:
        f.write(f"# {title}\n\n{formatted_text}\n")

    return transcript_path


def transcribe_and_format(audio_path: Path, on_update=None, output_dir: str = "output", timing: dict | None = None):
    """
    èåˆæµæ°´çº¿ï¼šWhisper è½¬å½•ä¸é€šä¹‰åƒé—®æ ¼å¼åŒ–å¹¶è¡Œè¿›è¡Œã€‚
    æ¯æ£€æµ‹åˆ°è¯­éŸ³åœé¡¿å°±åˆ‡å‡ºä¸€æ®µï¼Œç«‹åˆ»ä¸¢ç»™é€šä¹‰åƒé—®åå°æ ¼å¼åŒ–ï¼ŒWhisper ç»§ç»­è½¬å½•ä¸‹ä¸€æ®µã€‚
    æ ¼å¼åŒ–å®Œæˆåï¼ŒAI è‡ªåŠ¨åˆ†æå…¨æ–‡å†…å®¹ç”Ÿæˆç»“æ„åŒ–æ ‡é¢˜ã€‚

    on_update(content, formatted_count, total_paragraphs)
        - content: å½“å‰åº”æ˜¾ç¤ºçš„å®Œæ•´ Markdownï¼ˆå·²æ•´ç† + æœªæ•´ç† + æ­£åœ¨è½¬å½•ï¼‰
        - formatted_count: å·²å®Œæˆ AI æ•´ç†çš„æ®µè½æ•°
        - total_paragraphs: å·²åˆ‡å‡ºçš„æ®µè½æ€»æ•°

    timing: å¯é€‰å­—å…¸ï¼Œå‡½æ•°ä¼šå°†å„é˜¶æ®µè€—æ—¶å†™å…¥å…¶ä¸­
    """
    if timing is None:
        timing = {}
    Path(output_dir).mkdir(exist_ok=True)
    title = audio_path.stem.replace("_", " ")

    client = _get_llm_client()

    t0 = time.time()
    model = _get_whisper_model()
    timing["model_load"] = round(time.time() - t0, 1)
    print(f"â±ï¸ æ¨¡å‹åŠ è½½è€—æ—¶: {timing['model_load']}s")
    print("âœ… æ¨¡å‹å°±ç»ªï¼Œå¼€å§‹è½¬å½•+æ ¼å¼åŒ–æµæ°´çº¿...")

    t_whisper_start = time.time()
    segments, _info = model.transcribe(
        str(audio_path), language="zh",
        beam_size=1,        # è´ªå¿ƒè§£ç ï¼Œå¤§å¹…æé€Ÿï¼Œä¸­æ–‡è¯­éŸ³è´¨é‡æŸå¤±æå°
        vad_filter=True,    # è·³è¿‡é™éŸ³/éè¯­éŸ³æ®µï¼Œå‡å°‘æ— æ•ˆè½¬å½•
    )

    GAP_THRESHOLD = 1.0
    MAX_PARAGRAPH_CHARS = 500   # é€šä¹‰åƒé—®é€Ÿç‡é™åˆ¶å®½æ¾ï¼ˆ3ä¸‡RPMï¼‰ï¼Œå¯ç»†ç²’åº¦åˆ†æ®µæå‡å“åº”é€Ÿåº¦

    # â”€â”€ æ®µè½çŠ¶æ€ â”€â”€
    paragraphs = []         # [{"raw": str, "formatted": str|None}, ...]
    current_parts = []      # å½“å‰æ­£åœ¨è½¬å½•çš„æ®µè½ç‰‡æ®µ
    prev_end = 0.0
    formatted_count = 0

    # â”€â”€ ç« èŠ‚æ ‡é¢˜æ˜ å°„ï¼ˆç”± AI åœ¨æ ¼å¼åŒ–å®Œæˆåè‡ªåŠ¨ç”Ÿæˆï¼‰â”€â”€
    chapter_headers = {}    # {paragraph_index: "æ ‡é¢˜"}

    executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)
    pending_futures = {}    # {paragraph_index: Future}

    def _build_content():
        """ç»„è£…å½“å‰åº”æ˜¾ç¤ºçš„å®Œæ•´å†…å®¹ï¼ˆå«ç« èŠ‚æ ‡é¢˜ï¼‰"""
        parts = []
        for i, p in enumerate(paragraphs):
            if i in chapter_headers:
                parts.append(f"## {chapter_headers[i]}")
            parts.append(p["formatted"] if p["formatted"] else p["raw"])
        if current_parts:
            parts.append("".join(current_parts))
        return f"# {title}\n\n" + "\n\n".join(parts)

    def _check_futures():
        """æ£€æŸ¥å·²å®Œæˆçš„é€šä¹‰åƒé—®æ ¼å¼åŒ–ä»»åŠ¡"""
        nonlocal formatted_count
        changed = False
        for idx in list(pending_futures.keys()):
            future = pending_futures[idx]
            if future.done():
                try:
                    paragraphs[idx]["formatted"] = future.result()
                except Exception as e:
                    print(f"  âš ï¸ æ®µè½ {idx+1} æ ¼å¼åŒ–å¤±è´¥: {e}")
                formatted_count += 1
                changed = True
                del pending_futures[idx]
        return changed

    def _submit_paragraph():
        """å°†å½“å‰æ®µè½æäº¤ç»™é€šä¹‰åƒé—®æ ¼å¼åŒ–"""
        if not current_parts:
            return
        raw = "".join(current_parts)
        idx = len(paragraphs)
        paragraphs.append({"raw": raw, "formatted": None})
        current_parts.clear()
        future = executor.submit(
            _call_llm, client, raw,
            part_info=f"æ®µè½{idx+1} ",
            prompt_template=SEGMENT_PROMPT,
        )
        pending_futures[idx] = future
        print(f"  ğŸ“¤ æ®µè½ {idx+1} å·²æäº¤é€šä¹‰åƒé—®ï¼ˆ{len(raw)} å­—ï¼‰")

    def _notify():
        if on_update:
            on_update(_build_content(), formatted_count, len(paragraphs))

    # â”€â”€ ä¸»å¾ªç¯ï¼šWhisper è½¬å½• â”€â”€
    for segment in segments:
        text = segment.text.strip()
        if not text:
            continue

        gap = segment.start - prev_end
        current_len = sum(len(p) for p in current_parts)

        # æ£€æµ‹åˆ°åœé¡¿ æˆ– æ®µè½è¿‡é•¿ â†’ åˆ‡æ®µ â†’ æäº¤æ ¼å¼åŒ–
        if current_parts and (gap >= GAP_THRESHOLD or current_len >= MAX_PARAGRAPH_CHARS):
            reason = f"åœé¡¿ {gap:.1f}s" if gap >= GAP_THRESHOLD else f"å·²è¾¾ {current_len} å­—"
            print(f"  --- {reason}ï¼Œåˆ†æ®µ ---")
            _submit_paragraph()

        # é¡ºä¾¿æ£€æŸ¥å·²å®Œæˆçš„æ ¼å¼åŒ–
        _check_futures()

        print(f"  [{segment.start:.1f}s - {segment.end:.1f}s] {text}")
        current_parts.append(text)
        prev_end = segment.end

        _notify()

    # â”€â”€ æäº¤æœ€åä¸€æ®µ â”€â”€
    _submit_paragraph()
    _check_futures()
    _notify()

    timing["whisper"] = round(time.time() - t_whisper_start, 1)
    print(f"â±ï¸ Whisper è½¬å½•è€—æ—¶: {timing['whisper']}sï¼ˆ{len(paragraphs)} ä¸ªæ®µè½ï¼‰")

    # â”€â”€ ç­‰å¾…æ‰€æœ‰æ ¼å¼åŒ–å®Œæˆ â”€â”€
    t_format_wait = time.time()
    print("  â³ ç­‰å¾…å‰©ä½™é€šä¹‰åƒé—®æ ¼å¼åŒ–å®Œæˆ...")
    for idx in sorted(pending_futures.keys()):
        future = pending_futures[idx]
        try:
            paragraphs[idx]["formatted"] = future.result()
        except Exception as e:
            print(f"  âš ï¸ æ®µè½ {idx+1} æ ¼å¼åŒ–å¤±è´¥: {e}")
        formatted_count += 1
        _notify()

    pending_futures.clear()
    timing["ai_format"] = round(time.time() - t_format_wait, 1)
    print(f"â±ï¸ AI æ ¼å¼åŒ–ç­‰å¾…è€—æ—¶: {timing['ai_format']}s")

    # â”€â”€ é‡è¯•æ‰€æœ‰æ ¼å¼åŒ–å¤±è´¥çš„æ®µè½ â”€â”€
    t_retry = time.time()
    failed_indices = [i for i, p in enumerate(paragraphs) if p["formatted"] is None]
    if failed_indices:
        print(f"\nğŸ”„ {len(failed_indices)} ä¸ªæ®µè½æ ¼å¼åŒ–å¤±è´¥ï¼Œç­‰å¾… 15s åé›†ä¸­é‡è¯•...")
        time.sleep(15)
        for idx in failed_indices:
            try:
                print(f"  ğŸ”„ é‡è¯•æ®µè½ {idx+1}ï¼ˆ{len(paragraphs[idx]['raw'])} å­—ï¼‰...")
                result = _call_llm(client, paragraphs[idx]["raw"],
                                   f"æ®µè½{idx+1} ", SEGMENT_PROMPT)
                paragraphs[idx]["formatted"] = result
                print(f"  âœ… æ®µè½ {idx+1} é‡è¯•æˆåŠŸ")
                _notify()
            except Exception as e:
                print(f"  âŒ æ®µè½ {idx+1} é‡è¯•ä»ç„¶å¤±è´¥: {e}")
        # äºŒæ¬¡é‡è¯•ä»å¤±è´¥çš„æ®µè½
        still_failed = [i for i in failed_indices if paragraphs[i]["formatted"] is None]
        if still_failed:
            print(f"  âš ï¸ ä»æœ‰ {len(still_failed)} ä¸ªæ®µè½æœªèƒ½æ ¼å¼åŒ–ï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡æœ¬")

    timing["retry"] = round(time.time() - t_retry, 1)
    if failed_indices:
        print(f"â±ï¸ é‡è¯•è€—æ—¶: {timing['retry']}s")

    executor.shutdown(wait=False)

    # â”€â”€ ç”¨ AI è‡ªåŠ¨åˆ†æå†…å®¹ï¼Œç”Ÿæˆç»“æ„åŒ–æ ‡é¢˜ â”€â”€
    t_structure = time.time()
    if len(paragraphs) >= 3:
        print("  ğŸ“‘ æ­£åœ¨ç”¨ AI åˆ†æå†…å®¹ï¼Œç”Ÿæˆç»“æ„æ ‡é¢˜...")
        try:
            ai_headers = _generate_section_headers(client, paragraphs)
            if ai_headers:
                chapter_headers.update(ai_headers)
                print(f"  âœ… AI ç”Ÿæˆäº† {len(ai_headers)} ä¸ªç»“æ„æ ‡é¢˜:")
                for idx in sorted(ai_headers):
                    print(f"     æ®µè½ {idx+1}: {ai_headers[idx]}")
                _notify()
            else:
                print("  â„¹ï¸ AI æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ç»“æ„æ ‡é¢˜")
        except Exception as e:
            print(f"  âš ï¸ ç»“æ„æ ‡é¢˜ç”Ÿæˆå¤±è´¥ï¼ˆä¸å½±å“å†…å®¹ï¼‰: {e}")

    timing["structure"] = round(time.time() - t_structure, 1)
    if timing["structure"] > 0.1:
        print(f"â±ï¸ ç»“æ„æ ‡é¢˜ç”Ÿæˆè€—æ—¶: {timing['structure']}s")

    # â”€â”€ ä¿å­˜æœ€ç»ˆæ–‡ä»¶ â”€â”€
    final_content = _build_content() + "\n"
    transcript_path = Path(output_dir) / f"{audio_path.stem}.md"
    with open(transcript_path, "w", encoding="utf-8") as f:
        f.write(final_content)

    print(f"âœ… è½¬å½•+æ ¼å¼åŒ–å…¨éƒ¨å®Œæˆ: {transcript_path}")
    return transcript_path, final_content


def transcribe(audio_path: Path, output_dir: str = "output"):
    """å®Œæ•´æµç¨‹ï¼ˆCLI å…¼å®¹ï¼‰ï¼šè½¬å½• + AI æ ¼å¼åŒ–"""
    raw_text = transcribe_audio(audio_path)
    return format_transcript(raw_text, audio_path, output_dir)
